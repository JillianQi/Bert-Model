{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT implemented from scratch\n",
    "\n",
    "In this file, I implemented BERT from scratch, focusing on the core concepts of transformer architecture, attention mechanisms, and matrix operations. BERT (Bidirectional Encoder Representations from Transformers) is a pivotal model in NLP that uses a bidirectional approach to understand the context of words in a sentence.\n",
    "\n",
    "<br>\n",
    "\n",
    "I will also demonstrate how to load tensors directly from the pre-trained BERT model provided by Hugging Face. Before running this file, you need to download the weights. Here is the official link to download the weights: [Hugging Face BERT](https://huggingface.co/bert-base-uncased)\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/all-steps.png\"/ width=800>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Input Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "I won’t be implementing a BPE tokenizer here, but Andrej Karpathy has a neat implementation that you might find useful.\n",
    "<br>\n",
    "You can check out his implementation at this link: https://github.com/karpathy/minbpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "model_path=\"bert-base-uncased\"\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7592, 2088, 999, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world! [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Inspecting Model Configuration and Weights\n",
    "\n",
    "Here, the model configuration and weights are loaded from a pretrained model file. This step ensures that the necessary parameters are ready for the forward pass.\n",
    "\n",
    "Normally, reading a model file depends on how the model classes are written and the variable names within them. \n",
    "However, since we are implementing BERT from scratch, we will read the file one tensor at a time, carefully examining the embedding layers, attention heads, and feed-forward networks that make up BERT’s architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jlpanc/anaconda3/envs/codef/STA/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the pre-trained model using transformers\n",
    "\n",
    "# Load model configuration and weights\n",
    "model = BertForMaskedLM.from_pretrained(model_path, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# float32\n",
    "model['bert.embeddings.word_embeddings.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['BertForMaskedLM'],\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'gradient_checkpointing': False,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'layer_norm_eps': 1e-12,\n",
       " 'max_position_embeddings': 512,\n",
       " 'model_type': 'bert',\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'pad_token_id': 0,\n",
       " 'position_embedding_type': 'absolute',\n",
       " 'transformers_version': '4.6.0.dev0',\n",
       " 'type_vocab_size': 2,\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 30522}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"/home/jlpanc/anaconda3/envs/codef/STA/bert-base-uncased/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Model Parameters from Config\n",
    "We use this config to infer details about the model like:\n",
    "\n",
    "1. The model has 12 transformer layers.\n",
    "2. Each multi-head attention block has 12 heads.\n",
    "3. The vocabulary size is 30,522 tokens.\n",
    "4. The hidden size is 768 dimensions.\n",
    "5. The intermediate size (for the feed-forward layers) is 3072 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = config[\"hidden_size\"]\n",
    "n_layers = config[\"num_hidden_layers\"]\n",
    "n_heads = config[\"num_attention_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "norm_eps = config[\"layer_norm_eps\"]\n",
    "eps = config[\"layer_norm_eps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting text to tokens\n",
    "Here, we utilize the tiktoken library (which I believe is developed by OpenAI) as the tokenizer.\n",
    "<div>\n",
    "    <img src=\"images/embedding_layers.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  tensor([[ 101, 2043, 1999, 4199, 1010, 2079, 2004, 1996,  103, 2079, 1012,  102]])\n",
      "token_type_ids:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "q_len:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_549748/3428054749.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids[0])\n"
     ]
    }
   ],
   "source": [
    "# 1. Tokenize the prompt into token IDs using the BERT tokenizer.\n",
    "prompt = \"When in Rome, do as the [MASK] do.\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "print(\"input_ids: \", input_ids)\n",
    "# 2. Create a tensor of token type IDs (all zeros, as we have only one sentence).\n",
    "token_type_ids = torch.zeros_like(input_ids)\n",
    "print(\"token_type_ids: \", token_type_ids)\n",
    "# 3. Convert the input_ids to a tensor (this step is redundant since it's already a tensor).\n",
    "input_ids = torch.tensor(input_ids[0])\n",
    "# 4. Calculate and print the number of tokens in the input prompt.\n",
    "q_len = len(input_ids)\n",
    "print(\"q_len: \", q_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Token Embeddings\n",
    "In this section, we convert input tokens into their corresponding embeddings using BERT’s pre-trained embedding layers. While BERT uses inbuilt neural network modules for this process, it’s crucial to understand the transformation.\n",
    "\n",
    "So, our [10x1] tokens are now transformed into [10x768], i.e., 12 embeddings (one for each token) of length 768.\n",
    "\n",
    "Note: Keep track of the tensor shapes throughout the process; it makes understanding the entire architecture much easier.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/embeddings_output.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the word embeddings from the pre-trained BERT model\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(model['bert.embeddings.word_embeddings.weight'])\n",
    "# 2. Load the position embeddings from the pre-trained BERT model\n",
    "position_embeddings = torch.nn.Embedding.from_pretrained(model['bert.embeddings.position_embeddings.weight'])\n",
    "# 3. Load the segment (token type) embeddings from the pre-trained BERT model\n",
    "segment_embeddings = torch.nn.Embedding.from_pretrained(model['bert.embeddings.token_type_embeddings.weight'])\n",
    "# embedding_layer.weight.data.copy_(model[\"model.embed_tokens.weight\"])\n",
    "token_embeddings_unnormalized = embedding_layer(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
       "        [-0.0463,  0.0259, -0.0240,  ..., -0.0416,  0.0289,  0.0316],\n",
       "        [-0.0278,  0.0039, -0.0035,  ...,  0.0078, -0.0306,  0.0221],\n",
       "        ...,\n",
       "        [ 0.0066, -0.0533,  0.0057,  ..., -0.0140, -0.0522, -0.0088],\n",
       "        [-0.0207, -0.0020, -0.0118,  ...,  0.0128,  0.0200,  0.0259],\n",
       "        [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(q_len, dtype=torch.long).unsqueeze(0)\n",
    "position_embeds = position_embeddings(position_ids)\n",
    "position_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_embeds = segment_embeddings(token_type_ids)\n",
    "segment_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_unnormalized = token_embeddings_unnormalized+position_embeds+segment_embeds\n",
    "embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0316, -0.0411, -0.0564,  ...,  0.0021,  0.0044,  0.0219],\n",
       "         [-0.0381,  0.0392, -0.0397,  ..., -0.0193,  0.0553,  0.0177],\n",
       "         [-0.0387,  0.0129, -0.0114,  ...,  0.0161, -0.0153,  0.0062],\n",
       "         ...,\n",
       "         [ 0.0135, -0.0457, -0.0071,  ...,  0.0045, -0.0567, -0.0130],\n",
       "         [-0.0299,  0.0069, -0.0199,  ...,  0.0310,  0.0295,  0.0295],\n",
       "         [-0.0253,  0.0052, -0.0068,  ..., -0.0129,  0.0153, -0.0015]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Layer Normalization\n",
    "In BERT, we apply Layer Normalization after embedding layers and various other stages.\n",
    "Note: This step does not change the shape of the tensor; it just normalizes the values across the hidden dimension.\n",
    "\n",
    "### Things to keep in mind:\n",
    "1. Layer normalization prevents internal covariate shift by normalizing across the features.\n",
    "2. A small epsilon value (from the configuration) is added for numerical stability, preventing division by zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the First Layer of the Transformer\n",
    "\n",
    "#### Normalization\n",
    "In BERT, we start by applying layer normalization to the input of each transformer layer.\n",
    "\n",
    "You will notice that I access `encoder.layer.0` from the model dictionary (this corresponds to the first layer in BERT).\n",
    "After applying the normalization, the tensor shape remains the same as the input, [12x768], but the values are now normalized.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/layer_norm.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, gamma, beta, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    x (torch.Tensor): Input tensor with shape (batch_size, num_features)\n",
    "    gamma (torch.Tensor): Scale parameter, with the same shape as the last dimension of x\n",
    "    beta (torch.Tensor): Shift parameter, with the same shape as the last dimension of x\n",
    "    eps (float): A small constant for numerical stability (to prevent division by zero)\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The output after applying LayerNorm\n",
    "    \"\"\"\n",
    "    # Calculate the mean and variance\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    \n",
    "    # Normalization\n",
    "    x_normalized = (x - mean) / torch.sqrt(variance + eps)\n",
    "    \n",
    "    # Scaling and Shifting\n",
    "    out = gamma * x_normalized + beta\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the first transformer layer\n",
    "### Normalization\n",
    "You can see, after through layer0 from the dict extract from the model.\n",
    "\n",
    "The output tensor is still shape in [10*768] but normalized.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/embeding_norm1.png\", width=500>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings after LayerNorm: tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [-0.4807,  0.8188, -0.4227,  ..., -0.1415,  1.1064,  0.5430],\n",
      "         [-0.4992,  0.3891,  0.0274,  ...,  0.3810, -0.0397,  0.3439],\n",
      "         ...,\n",
      "         [ 0.5077, -0.5198,  0.1786,  ...,  0.2883, -0.6555,  0.0927],\n",
      "         [-0.3585,  0.2777, -0.1210,  ...,  0.5949,  0.6856,  0.7453],\n",
      "         [-0.4771,  0.0871, -0.0770,  ..., -0.2191,  0.3020,  0.0196]]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize LayerNorm and Dropout\n",
    "# layer_norm = torch.nn.LayerNorm(768, eps=1e-12)\n",
    "# layer_norm.weight.data = model['bert.embeddings.LayerNorm.weight']\n",
    "# layer_norm.bias.data = model['bert.embeddings.LayerNorm.bias']\n",
    "\n",
    "# Perform Layer Normalization\n",
    "normalized_embeddings = layer_norm(embeddings_unnormalized, model['bert.embeddings.LayerNorm.weight'], model['bert.embeddings.LayerNorm.bias'], eps=1e-12)\n",
    "\n",
    "# Apply Dropout\n",
    "# final_embeddings = dropout(normalized_embeddings)\n",
    "\n",
    "# Print results\n",
    "print(\"Embeddings after LayerNorm:\", normalized_embeddings)\n",
    "# print(\"Final Embeddings:\", final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism\n",
    "This section dives into how the self-attention mechanism is implemented in BERT, including the computation of Query, Key, and Value matrices, and how they contribute to generating attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the query, key, and value weights for the first attention layer from the BERT model\n",
    "q_layer0 = model[\"bert.encoder.layer.0.attention.self.query.weight\"]\n",
    "k_layer0 = model[\"bert.encoder.layer.0.attention.self.key.weight\"]\n",
    "v_layer0 = model[\"bert.encoder.layer.0.attention.self.value.weight\"]\n",
    "# Extract the biases for query, key, and value in the first attention layer\n",
    "q_layer0_bias = model['bert.encoder.layer.0.attention.self.query.bias']\n",
    "k_layer0_bias = model['bert.encoder.layer.0.attention.self.key.bias']\n",
    "v_layer0_bias = model['bert.encoder.layer.0.attention.self.value.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the query, key, and value states by multiplying the normalized embeddings with the respective weights and adding the bias\n",
    "query_states = torch.matmul(normalized_embeddings, q_layer0.T)+q_layer0_bias\n",
    "key_states = torch.matmul(normalized_embeddings, k_layer0.T)+k_layer0_bias\n",
    "value_states = torch.matmul(normalized_embeddings, v_layer0.T)+v_layer0_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to reshape and permute the states for multi-head attention\n",
    "def transpose_for_scores(x):\n",
    "    new_x_shape = x.size()[:-1] + (12, 64) # (num_attention_heads, attention_head_size)\n",
    "    x = x.view(new_x_shape) # Reshape the tensor to separate attention heads\n",
    "    return x.permute(0, 2, 1, 3) # Permute the tensor dimensions for attention computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaination of multi-head attention\n",
    "Multi-head attention is a core component of the BERT architecture, enabling the model to focus on different parts of the input sequence simultaneously. Instead of calculating attention just once, multi-head attention performs the attention mechanism multiple times in parallel, each time using different learned linear projections of the query, key, and value vectors. This process creates multiple “heads” of attention, with each head focusing on different aspects of the input.\n",
    "\n",
    "In BERT, each attention head operates independently, capturing diverse relationships between tokens by attending to different positions in the sequence. After computing attention for each head, the results are concatenated and passed through a final linear transformation to produce the output. This approach enriches BERT’s ability to understand context by integrating information from multiple attention perspectives, making it highly effective in natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transpose_for_scores function to the query, key, and value states\n",
    "transposed_query_states = transpose_for_scores(query_states)\n",
    "transposed_key_states = transpose_for_scores(key_states)\n",
    "transposed_value_states = transpose_for_scores(value_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transposed_query_states shape:  torch.Size([1, 12, 12, 64])\n",
      "transposed_key_states shape:  torch.Size([1, 12, 12, 64])\n",
      "transposed_value_states shape:  torch.Size([1, 12, 12, 64])\n"
     ]
    }
   ],
   "source": [
    "print('transposed_query_states shape: ',transposed_query_states.shape)\n",
    "print('transposed_key_states shape: ',transposed_key_states.shape)\n",
    "print('transposed_value_states shape: ',transposed_value_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Attention Score\n",
    "We then multiply the queries and key matrices in the process known as self-attention:\n",
    "\n",
    "- Performing this operation yields a score that maps the relationship between each token and every other token in the sequence. \n",
    "- This score indicates how well each token’s query aligns with every other token’s key. \n",
    "- The resulting attention score matrix (referred to as qk_per_token) has a shape of [12x12], where 12 represents the number of tokens in the input sequence.\n",
    "\n",
    "\n",
    "#### Attention Implemented from Scratch\n",
    "\n",
    "Let's load the attention heads of the first layer of the transformer.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/qkv.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "### Loading the Query, Key, Value, and Output Matrices\n",
    "\n",
    "When we load the query, key, value, and output matrices from the model, we notice that their shapes are:\n",
    "- Query: [768x768]\n",
    "- Key: [768x768]\n",
    "- Value: [768x768]\n",
    "- Output: [768x768]\n",
    "\n",
    "At first glance, this might seem unusual because ideally, we would want separate q, k, v, and o matrices for each attention head individually.\n",
    "\n",
    "However, BERT’s authors bundled these matrices together for efficiency—it allows parallelization of the attention head calculations.\n",
    "\n",
    "Now, let's unwrap these bundled matrices to access each attention head individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the attention output using scaled dot-product attention (built-in function)\n",
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    t_query_states,\n",
    "    t_key_states,\n",
    "    t_value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p= 0.0,\n",
    "    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "    is_causal= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and transpose the attention output to match expected dimensions\n",
    "attn_output = attn_output.transpose(1, 2).reshape(1, 12, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the output dense layer weights and biases for the first attention layer\n",
    "o_layer0 = model[\"bert.encoder.layer.0.attention.output.dense.weight\"]\n",
    "o_layer0_bias = model['bert.encoder.layer.0.attention.output.dense.bias']\n",
    "# Compute the final attention output states by applying the dense layer transformation\n",
    "ouput_states = torch.matmul(attn_output, o_layer0.T)+o_layer0_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0469,  0.1639, -0.0603,  ..., -0.1527,  0.0335,  0.1802],\n",
       "         [ 0.1023,  0.3846, -0.0411,  ..., -0.5442, -0.0701, -0.0260],\n",
       "         [-0.0404,  0.2899,  0.1311,  ..., -0.2009, -0.2455, -0.0298],\n",
       "         ...,\n",
       "         [ 0.2090,  0.3050,  0.0050,  ..., -0.2973, -0.1212, -0.0459],\n",
       "         [ 0.0831, -0.0208, -0.1554,  ..., -0.4028, -0.2934, -0.0784],\n",
       "         [ 0.3431, -0.1718,  0.0026,  ..., -0.3566, -0.2113,  0.0350]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ouput_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ouput_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the LayerNorm weights and biases for the first attention layer's output\n",
    "layer_0_output_norm_weight = model['bert.encoder.layer.0.attention.output.LayerNorm.weight']\n",
    "layer_0_output_norm_bias = model['bert.encoder.layer.0.attention.output.LayerNorm.bias']\n",
    "# Apply LayerNorm to the output states combined with the input embeddings\n",
    "attention_output = torch.nn.functional.layer_norm(ouput_states+normalized_embeddings, [ouput_states.size(-1)], layer_0_output_norm_weight, layer_0_output_norm_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "In the context of transformer models like BERT, upsampling refers to techniques that increase the resolution or dimensionality of representations, ensuring that the model can capture and retain more detailed information across different layers. Although traditional upsampling methods like bilinear interpolation and transposed convolution are more commonly associated with image processing tasks, the concept can be loosely related to operations in NLP models where the dimensionality of embeddings is increased or preserved to enrich the feature space. This process is crucial in enabling the model to maintain high levels of information density, allowing subsequent layers, such as the feed-forward neural network, to process richer and more complex representations, ultimately improving the model’s ability to understand and generate nuanced text.\n",
    "\n",
    "## Feed-Forward Neural Network\n",
    "After the self-attention mechanism, each transformer layer in BERT applies a feed-forward neural network to further process the embeddings. This step is crucial for introducing non-linearity and enabling the model to learn complex representations from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.1348, -2.2779, -3.1132,  ..., -2.4296, -3.1535, -2.9147],\n",
       "         [-1.7857, -1.4517, -3.6339,  ..., -1.9001, -2.6653,  0.6086],\n",
       "         [-3.0365, -0.8983, -4.2322,  ..., -2.2538, -2.9196, -2.1696],\n",
       "         ...,\n",
       "         [-1.7389, -2.1817, -4.1068,  ..., -1.2588, -1.4242, -1.0926],\n",
       "         [-2.9177, -1.5600, -3.8377,  ..., -1.5096, -2.2996, -1.7339],\n",
       "         [-3.7864, -2.3012, -3.4133,  ..., -2.6182, -2.6048, -3.1074]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the weights and biases for the intermediate dense layer of the first transformer layer\n",
    "layer_0_intermediate_dense_weight = model['bert.encoder.layer.0.intermediate.dense.weight']\n",
    "layer_0_intermediate_dense_bias = model['bert.encoder.layer.0.intermediate.dense.bias']\n",
    "# Compute the output of the intermediate dense layer\n",
    "intermediate_dense_output = torch.matmul(attention_output, layer_0_intermediate_dense_weight.T)+layer_0_intermediate_dense_bias\n",
    "intermediate_dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 3072])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_dense_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "After the intermediate dense layer computes its output, an activation function is applied to introduce non-linearity into the model. In BERT, the GELU (Gaussian Error Linear Unit) activation function is used. Unlike the more common ReLU (Rectified Linear Unit), GELU is a smooth, continuous function that models the output as a probabilistic approximation, allowing for both positive and negative values. This property helps BERT to better capture the nuanced relationships in the input data, leading to improved performance on a variety of NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.6957e-03, -2.5889e-02, -2.8811e-03,  ..., -1.8364e-02,\n",
       "          -2.5440e-03, -5.1889e-03],\n",
       "         [-6.6208e-02, -1.0640e-01, -5.0749e-04,  ..., -5.4553e-02,\n",
       "          -1.0250e-02,  4.4340e-01],\n",
       "         [-3.6334e-03, -1.6575e-01, -4.8812e-05,  ..., -2.7281e-02,\n",
       "          -5.1157e-03, -3.2584e-02],\n",
       "         ...,\n",
       "         [-7.1338e-02, -3.1777e-02, -8.2369e-05,  ..., -1.3097e-01,\n",
       "          -1.0994e-01, -1.5000e-01],\n",
       "         [-5.1445e-03, -9.2632e-02, -2.3813e-04,  ..., -9.8983e-02,\n",
       "          -2.4687e-02, -7.1898e-02],\n",
       "         [-2.8956e-04, -2.4598e-02, -1.0957e-03,  ..., -1.1571e-02,\n",
       "          -1.1973e-02, -2.9320e-03]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the GELU activation function to the intermediate dense output\n",
    "intermediate_output = torch.nn.functional.gelu(intermediate_dense_output)\n",
    "intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 3072])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "In the context of transformer models like BERT, downsampling refers to techniques that reduce the resolution or dimensionality of representations, ensuring that the model can process information more efficiently while retaining the most critical features. Although traditional downsampling methods like pooling or strided convolutions are more commonly associated with image processing tasks, the concept can be loosely applied to operations in NLP models where the dimensionality of embeddings is reduced to streamline the feature space. This process is crucial in managing the computational load of the model, allowing subsequent layers, such as the feed-forward neural network, to focus on the most relevant information. By condensing the data, downsampling helps the model maintain a balance between complexity and efficiency, ultimately enhancing its ability to process large input sequences and generate accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_0_intermediate_dense_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the weights and biases for the output dense layer of the first transformer layer\n",
    "layer_0_output_dense_weight = model['bert.encoder.layer.0.output.dense.weight']\n",
    "layer_0_output_dense_bias = model['bert.encoder.layer.0.output.dense.bias']\n",
    "# Compute the final dense layer output for the first transformer layer\n",
    "output_dense = torch.matmul(intermediate_output, layer_0_output_dense_weight.T)+layer_0_output_dense_bias\n",
    "output_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the LayerNorm weights and biases for the output of the first transformer layer\n",
    "layer_0_output_layernorm_weight = model['bert.encoder.layer.0.output.LayerNorm.weight']\n",
    "layer_0_output_layernorm_bias = model['bert.encoder.layer.0.output.LayerNorm.bias']\n",
    "# Apply LayerNorm to the sum of the dense layer output and the original attention output\n",
    "layer_norm(output_dense+attention_output, layer_0_output_layernorm_weight, layer_0_output_layernorm_bias, eps=1e-12).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass Through BERT’s 12 Transformer Layers\n",
    "Next, we run a forward pass through all 12 layers of the BERT model. \n",
    "\n",
    "Each layer applies multi-head self-attention to capture token relationships, followed by a dense layer with GELU activation for non-linearity, and then Layer Normalization to stabilize the output. \n",
    "\n",
    "The output from each layer is passed to the next, and after processing all 12 layers, we obtain a final contextualized embedding ready for downstream tasks like classification or question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## god, everything all at once\n",
    "<div>\n",
    "    <img src=\"images/12self_attention.png\" width=\"600px\"/>\n",
    "</div>\n",
    "yep, this is it. everything we did before, all at once, for every single layer.\n",
    "<br>\n",
    "\n",
    "### have fun reading :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0698,  0.3857, -0.0319,  ..., -0.1526,  0.0446,  0.2253],\n",
       "         [-0.5065,  0.7490,  0.5782,  ..., -0.7123, -0.4653, -0.7150],\n",
       "         [-0.0510, -0.6486, -0.1926,  ..., -0.1028,  0.0320,  0.1524],\n",
       "         ...,\n",
       "         [ 0.8641,  0.3875,  0.5311,  ...,  0.2063,  0.1742,  0.1639],\n",
       "         [-0.4663, -0.5197, -0.2011,  ...,  0.6513,  0.0255, -0.1058],\n",
       "         [ 0.7993,  0.2471, -0.0632,  ...,  0.1179, -0.7094, -0.0537]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize with the normalized embeddings from the input layer\n",
    "layer_embedding_norm = normalized_embeddings\n",
    "# Iterate through all 12 transformer layers in BERT\n",
    "for layer in range(12):\n",
    "    # Extract query, key, and value weights for the current layer's attention mechanism\n",
    "    q_layer = model[f\"bert.encoder.layer.{layer}.attention.self.query.weight\"]\n",
    "    k_layer = model[f\"bert.encoder.layer.{layer}.attention.self.key.weight\"]\n",
    "    v_layer = model[f\"bert.encoder.layer.{layer}.attention.self.value.weight\"]\n",
    "    \n",
    "    # Extract biases for the query, key, and value\n",
    "    q_layer_bias = model[f'bert.encoder.layer.{layer}.attention.self.query.bias']\n",
    "    k_layer_bias = model[f'bert.encoder.layer.{layer}.attention.self.key.bias']\n",
    "    v_layer_bias = model[f'bert.encoder.layer.{layer}.attention.self.value.bias']\n",
    "    \n",
    "    # Compute query, key, and value states by applying the respective weights and biases\n",
    "    query_states = torch.matmul(layer_embedding_norm, q_layer.T)+q_layer_bias\n",
    "    key_states = torch.matmul(layer_embedding_norm, k_layer.T)+k_layer_bias\n",
    "    value_states = torch.matmul(layer_embedding_norm, v_layer.T)+v_layer_bias\n",
    "    \n",
    "    # Transpose and reshape states for multi-head attention computation\n",
    "    t_query_states = transpose_for_scores(query_states)\n",
    "    t_key_states = transpose_for_scores(key_states)\n",
    "    t_value_states = transpose_for_scores(value_states)\n",
    "    \n",
    "    # Compute the attention output using scaled dot-product attention\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        t_query_states,\n",
    "        t_key_states,\n",
    "        t_value_states,\n",
    "        attn_mask=None,\n",
    "        dropout_p= 0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal= False,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).reshape(1, 12, 768)\n",
    "    \n",
    "    # Extract the dense layer weights and biases for the attention output transformation\n",
    "    o_layer = model[f\"bert.encoder.layer.{layer}.attention.output.dense.weight\"]\n",
    "    o_layer_bias = model[f'bert.encoder.layer.{layer}.attention.output.dense.bias']\n",
    "    ouput_states = torch.matmul(attn_output, o_layer.T)+o_layer_bias\n",
    "    \n",
    "    # Extract LayerNorm weights and biases for the attention output\n",
    "    layer_output_norm_weight = model[f'bert.encoder.layer.{layer}.attention.output.LayerNorm.weight']\n",
    "    layer_output_norm_bias = model[f'bert.encoder.layer.{layer}.attention.output.LayerNorm.bias']\n",
    "    attention_output11 = layer_norm(ouput_states+layer_embedding_norm, layer_output_norm_weight, layer_output_norm_bias, eps=1e-12)\n",
    "\n",
    "    # Compute intermediate dense layer output\n",
    "    layer_intermediate_dense_weight = model[f'bert.encoder.layer.{layer}.intermediate.dense.weight']\n",
    "    layer_intermediate_dense_bias = model[f'bert.encoder.layer.{layer}.intermediate.dense.bias']\n",
    "    intermediate_dense_output = torch.matmul(attention_output11, layer_intermediate_dense_weight.T)+layer_intermediate_dense_bias\n",
    "    \n",
    "    # Apply GELU activation function\n",
    "    intermediate_output = torch.nn.functional.gelu(intermediate_dense_output)\n",
    "    \n",
    "    # Compute output dense layer transformation\n",
    "    layer_output_dense_weight = model[f'bert.encoder.layer.{layer}.output.dense.weight']\n",
    "    layer_output_dense_bias = model[f'bert.encoder.layer.{layer}.output.dense.bias']\n",
    "    output_dense = torch.matmul(intermediate_output, layer_output_dense_weight.T)+layer_output_dense_bias\n",
    "    \n",
    "    # Apply final LayerNorm for the current layer\n",
    "    layer_output_layernorm_weight = model[f'bert.encoder.layer.{layer}.output.LayerNorm.weight']\n",
    "    layer_output_layernorm_bias = model[f'bert.encoder.layer.{layer}.output.LayerNorm.bias']\n",
    "    layer_embedding_norm = layer_norm(output_dense+attention_output11, layer_output_layernorm_weight, layer_output_layernorm_bias, eps=1e-12)\n",
    "\n",
    "\n",
    "# Final output after all 12 layers of the transformer\n",
    "layer_embedding_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_embedding_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9308,  0.5810, -1.5488,  ..., -0.3969, -0.2049, -0.7886],\n",
       "         [-0.4524,  1.4434,  0.7572,  ...,  0.8591,  1.1784,  1.1476],\n",
       "         [ 0.1031,  0.8686,  0.9038,  ...,  1.2142,  0.3628,  1.5648],\n",
       "         ...,\n",
       "         [ 1.0442, -0.6966,  1.6925,  ...,  1.5027,  0.0384,  2.1948],\n",
       "         [-0.2648, -0.0293,  0.6970,  ...,  0.6369,  1.0992,  1.0511],\n",
       "         [ 0.1265, -1.1419,  0.2915,  ..., -0.2913,  0.0908,  0.3325]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract weights and biases for the dense layer in the prediction head\n",
    "layer_intermediate_dense_weight = model['cls.predictions.transform.dense.weight']\n",
    "layer_intermediate_dense_bias = model['cls.predictions.transform.dense.bias']\n",
    "\n",
    "# Compute the output of the dense layer in the prediction head\n",
    "prediction_output = torch.matmul(layer_embedding_norm, layer_intermediate_dense_weight.T)+layer_intermediate_dense_bias\n",
    "prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6974,  1.0231, -0.0350,  ..., -0.3549, -0.0176,  0.0247],\n",
       "         [-4.3757,  1.5879, -1.3361,  ..., -1.1796,  0.3573,  0.4541],\n",
       "         [-3.5006, -0.2887, -0.2252,  ...,  0.9825, -2.4059,  3.3587],\n",
       "         ...,\n",
       "         [-0.4326, -4.0172,  2.9924,  ...,  1.8936, -3.3367,  5.7582],\n",
       "         [-2.9313, -2.1162, -0.0482,  ..., -0.5154,  1.6312,  1.7295],\n",
       "         [-0.8187, -2.2804,  0.7585,  ..., -2.2679, -0.4951,  1.3419]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply GELU activation to the prediction output\n",
    "prediction_act_dense_output = torch.nn.functional.gelu(prediction_output)\n",
    "\n",
    "# Extract LayerNorm weights and biases for the prediction output\n",
    "prediction_layernorm_weight = model['cls.predictions.transform.LayerNorm.weight']\n",
    "prediction_layernorm_bias = model['cls.predictions.transform.LayerNorm.bias']\n",
    "# Apply LayerNorm to the activated dense output\n",
    "prediction_dense_output = layer_norm(prediction_act_dense_output, prediction_layernorm_weight, prediction_layernorm_bias, eps=1e-12)\n",
    "prediction_dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dense_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract decoder weights and biases for final prediction\n",
    "decoder_weight = model['cls.predictions.decoder.weight'] # 形状 [vocab_size, hidden_size]\n",
    "decoder_bias = model['cls.predictions.decoder.bias'] # 形状 [vocab_size]\n",
    "\n",
    "# Extract decoder weights and biases for final prediction\n",
    "# if 'cls.predictions.bias' in model:\n",
    "#     decoder_bias += model['cls.predictions.bias']\n",
    "\n",
    "# Project the final output to the vocabulary space to compute logits\n",
    "logits = torch.matmul(prediction_dense_output, decoder_weight.T) + decoder_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 30522])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取预测的标记 (词汇表中的索引)\n",
    "predictions = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2043, 1999, 4199, 1010, 2079, 2004, 1996,  103, 2079, 1012,  102]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets go！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. when in rome, do as the romans do..'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(predictions[0], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thank you\n",
    "\n",
    "This is the end. Hopefully you enjoyed reading it!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
