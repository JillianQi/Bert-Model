{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零实现BERT\n",
    "\n",
    "在此项目中，我将会演示如何从头拆解BERT，重点关注了Transformer架构的核心概念、注意力机制和矩阵运算。BERT（双向编码器表示来自Transformers）是NLP中的一个关键模型，它使用双向方法来理解句子中单词的上下文。\n",
    "\n",
    "我还将演示如何直接从Hugging Face提供的预训练BERT模型中加载张量。在运行此文件之前，你需要下载模型权重。以下是下载权重的官方链接：[Hugging Face BERT](https://huggingface.co/bert-base-uncased)\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/all-steps.png\"/ width=800>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型初始化与输入处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词\n",
    "\n",
    "我不会亲自在这里实现一个BPE分词器，但Andrej Karpathy有一个简洁的实现，如果感兴趣的话可以通过这个链接查看他的展示：https://github.com/karpathy/minbpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型加载\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "model_path=\"bert-base-uncased\"\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7592, 2088, 999, 102]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world! [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载和检查模型配置与权重\n",
    "\n",
    "在此步骤中，模型的配置和权重会从预训练模型文件中加载。此步骤确保在进行前向传播时，必要的参数已经准备就绪。\n",
    "\n",
    "通常，读取模型文件依赖于模型类的编写方式以及其中的变量名。然而，由于我们是从零开始实现BERT模型，我们将逐个读取张量，仔细检查BERT架构中的嵌入层、注意力头和前馈网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jlpanc/anaconda3/envs/codef/STA/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 1. 使用 transformers 加载预训练模型\n",
    "\n",
    "# 加载模型配置和权重\n",
    "model = BertForMaskedLM.from_pretrained(model_path, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# float32\n",
    "model['bert.embeddings.word_embeddings.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['BertForMaskedLM'],\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'gradient_checkpointing': False,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'layer_norm_eps': 1e-12,\n",
       " 'max_position_embeddings': 512,\n",
       " 'model_type': 'bert',\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'pad_token_id': 0,\n",
       " 'position_embedding_type': 'absolute',\n",
       " 'transformers_version': '4.6.0.dev0',\n",
       " 'type_vocab_size': 2,\n",
       " 'use_cache': True,\n",
       " 'vocab_size': 30522}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"/home/jlpanc/anaconda3/envs/codef/STA/bert-base-uncased/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "我们使用该配置推断模型的详细信息，如下所示：\n",
    "\n",
    "\t1.\t该模型有12个Transformer层。\n",
    "\t2.\t每个多头注意力模块有12个头。\n",
    "\t3.\t词汇表大小为30,522个token。\n",
    "\t4.\t隐藏层的维度为768维。\n",
    "\t5.\t前馈层的中间层大小为3072维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = config[\"hidden_size\"]\n",
    "n_layers = config[\"num_hidden_layers\"]\n",
    "n_heads = config[\"num_attention_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "norm_eps = config[\"layer_norm_eps\"]\n",
    "eps = config[\"layer_norm_eps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将文本转换为Token\n",
    "在这里，我们使用 tiktoken 库（由 OpenAI 开发）作为分词器。\n",
    "<div>\n",
    "    <img src=\"images/embedding_layers.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  tensor([[ 101, 2043, 1999, 4199, 1010, 2079, 2004, 1996,  103, 2079, 1012,  102]])\n",
      "token_type_ids:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "q_len:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_549748/3428054749.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids[0])\n"
     ]
    }
   ],
   "source": [
    "# 1. 使用BERT分词器将提示转换为token ID。\n",
    "prompt = \"When in Rome, do as the [MASK] do.\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "print(\"input_ids: \", input_ids)\n",
    "# 2. 创建一个token类型ID的张量（全为零，因为我们只有一个句子）。\n",
    "token_type_ids = torch.zeros_like(input_ids)\n",
    "print(\"token_type_ids: \", token_type_ids)\n",
    "# 3. 将input_ids转换为张量。\n",
    "input_ids = torch.tensor(input_ids[0])\n",
    "# 4. 计算并打印输入提示中的token数量。\n",
    "q_len = len(input_ids)\n",
    "print(\"q_len: \", q_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成Token嵌入\n",
    "\n",
    "在本节中，我们使用BERT的预训练嵌入层将输入的token转换为对应的嵌入。虽然BERT使用内置的神经网络模块来完成这一过程，但理解这种转换至关重要。\n",
    "\n",
    "因此，我们的[10x1] token现在被转换为[10x768]，也就是说，12个token（每个token一个）的嵌入长度为768。\n",
    "\n",
    "注意：在整个过程中请注意张量的形状变化，这将有助于更容易理解整个架构。\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/embeddings_output.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 从预训练的BERT模型中加载词嵌入\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(model['bert.embeddings.word_embeddings.weight'])\n",
    "# 2. 从预训练的BERT模型中加载位置嵌入\n",
    "position_embeddings = torch.nn.Embedding.from_pretrained(model['bert.embeddings.position_embeddings.weight'])\n",
    "# 3. 从预训练的BERT模型中加载分段（token类型）嵌入\n",
    "segment_embeddings = torch.nn.Embedding.from_pretrained(model['bert.embeddings.token_type_embeddings.weight'])\n",
    "# embedding_layer.weight.data.copy_(model[\"model.embed_tokens.weight\"])\n",
    "token_embeddings_unnormalized = embedding_layer(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
       "        [-0.0463,  0.0259, -0.0240,  ..., -0.0416,  0.0289,  0.0316],\n",
       "        [-0.0278,  0.0039, -0.0035,  ...,  0.0078, -0.0306,  0.0221],\n",
       "        ...,\n",
       "        [ 0.0066, -0.0533,  0.0057,  ..., -0.0140, -0.0522, -0.0088],\n",
       "        [-0.0207, -0.0020, -0.0118,  ...,  0.0128,  0.0200,  0.0259],\n",
       "        [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(q_len, dtype=torch.long).unsqueeze(0)\n",
    "position_embeds = position_embeddings(position_ids)\n",
    "position_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_embeds = segment_embeddings(token_type_ids)\n",
    "segment_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_unnormalized = token_embeddings_unnormalized+position_embeds+segment_embeds\n",
    "embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0316, -0.0411, -0.0564,  ...,  0.0021,  0.0044,  0.0219],\n",
       "         [-0.0381,  0.0392, -0.0397,  ..., -0.0193,  0.0553,  0.0177],\n",
       "         [-0.0387,  0.0129, -0.0114,  ...,  0.0161, -0.0153,  0.0062],\n",
       "         ...,\n",
       "         [ 0.0135, -0.0457, -0.0071,  ...,  0.0045, -0.0567, -0.0130],\n",
       "         [-0.0299,  0.0069, -0.0199,  ...,  0.0310,  0.0295,  0.0295],\n",
       "         [-0.0253,  0.0052, -0.0068,  ..., -0.0129,  0.0153, -0.0015]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 应用层归一化  \n",
    "在BERT中，我们在嵌入层和其他阶段之后应用层归一化。  \n",
    "提示：此步骤不会改变张量的形状，它只是对隐藏维度中的值进行归一化。\n",
    "\n",
    "### 需要额外注意的事项：\n",
    "1. 层归一化通过对特征进行归一化，防止内部协变量偏移。\n",
    "2. 为了数值稳定性（防止除以零），会添加一个小的epsilon值（从配置中获得）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建Transformer的第一层\n",
    "\n",
    "#### 归一化\n",
    "在BERT中，我们首先对每个Transformer层的输入应用层归一化。\n",
    "\n",
    "你会注意到，我从模型字典中访问`encoder.layer.0`（这对应于BERT的第一层）。  \n",
    "应用归一化后，张量的形状仍然与输入相同，为[12x768]，但数值现在已归一化。\n",
    "<div>\n",
    "    <img src=\"images/layer_norm.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, gamma, beta, eps=1e-12):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    x (torch.Tensor): 输入张量，形状为 (batch_size, num_features)\n",
    "    gamma (torch.Tensor): 缩放参数，形状与x的最后一个维度相同\n",
    "    beta (torch.Tensor): 偏移参数，形状与x的最后一个维度相同\n",
    "    eps (float): 用于数值稳定性的常量（防止除以零）\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 经过LayerNorm后的输出\n",
    "    \"\"\"\n",
    "    # 计算均值和方差\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    \n",
    "    # 标准化\n",
    "    x_normalized = (x - mean) / torch.sqrt(variance + eps)\n",
    "    \n",
    "    # 缩放和偏移\n",
    "    out = gamma * x_normalized + beta\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建第一个Transformer层  \n",
    "### 归一化  \n",
    "正如你所见，通过模型字典中的layer0后，输出的张量仍然是形状为[10*768]，但数值已经归一化了。\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/embeding_norm1.png\", width=500>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings after LayerNorm: tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [-0.4807,  0.8188, -0.4227,  ..., -0.1415,  1.1064,  0.5430],\n",
      "         [-0.4992,  0.3891,  0.0274,  ...,  0.3810, -0.0397,  0.3439],\n",
      "         ...,\n",
      "         [ 0.5077, -0.5198,  0.1786,  ...,  0.2883, -0.6555,  0.0927],\n",
      "         [-0.3585,  0.2777, -0.1210,  ...,  0.5949,  0.6856,  0.7453],\n",
      "         [-0.4771,  0.0871, -0.0770,  ..., -0.2191,  0.3020,  0.0196]]])\n"
     ]
    }
   ],
   "source": [
    "# 初始化 LayerNorm and Dropout\n",
    "# layer_norm = torch.nn.LayerNorm(768, eps=1e-12)\n",
    "# layer_norm.weight.data = model['bert.embeddings.LayerNorm.weight']\n",
    "# layer_norm.bias.data = model['bert.embeddings.LayerNorm.bias']\n",
    "\n",
    "# 执行层归一化\n",
    "normalized_embeddings = layer_norm(embeddings_unnormalized, model['bert.embeddings.LayerNorm.weight'], model['bert.embeddings.LayerNorm.bias'], eps=1e-12)\n",
    "\n",
    "# 应用Dropout\n",
    "# final_embeddings = dropout(normalized_embeddings)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Embeddings after LayerNorm:\", normalized_embeddings)\n",
    "# print(\"Final Embeddings:\", final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自注意力机制  \n",
    "本节深入探讨了BERT中自注意力机制的实现，包括Query、Key和Value矩阵的计算，以及它们如何共同生成注意力分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从BERT模型中提取第一个注意力层的query、key和value的权重\n",
    "q_layer0 = model[\"bert.encoder.layer.0.attention.self.query.weight\"]\n",
    "k_layer0 = model[\"bert.encoder.layer.0.attention.self.key.weight\"]\n",
    "v_layer0 = model[\"bert.encoder.layer.0.attention.self.value.weight\"]\n",
    "# 提取第一个注意力层中query、key和value的偏置\n",
    "q_layer0_bias = model['bert.encoder.layer.0.attention.self.query.bias']\n",
    "k_layer0_bias = model['bert.encoder.layer.0.attention.self.key.bias']\n",
    "v_layer0_bias = model['bert.encoder.layer.0.attention.self.value.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过将归一化后的嵌入与对应的权重相乘并加上偏置，计算query、key和value状态\n",
    "query_states = torch.matmul(normalized_embeddings, q_layer0.T)+q_layer0_bias\n",
    "key_states = torch.matmul(normalized_embeddings, k_layer0.T)+k_layer0_bias\n",
    "value_states = torch.matmul(normalized_embeddings, v_layer0.T)+v_layer0_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数用于对状态进行重塑和排列，以适应多头注意力\n",
    "def transpose_for_scores(x):\n",
    "    new_x_shape = x.size()[:-1] + (12, 64) # (num_attention_heads, attention_head_size)\n",
    "    x = x.view(new_x_shape) # Reshape the tensor to separate attention heads\n",
    "    return x.permute(0, 2, 1, 3) # Permute the tensor dimensions for attention computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力机制解释\n",
    "多头注意力是BERT架构中的一个核心组件，它使模型能够同时关注输入序列的不同部分。与只计算一次注意力不同，多头注意力会多次执行注意力机制（并行进行），每次使用不同的线性变换对query、key和value向量进行投影。这一过程产生了多个“头”的注意力，每个头都专注于输入的不同方面。\n",
    "\n",
    "在BERT中，每个注意力头独立运行，通过关注序列中不同位置的tokens，捕捉它们之间的多样关系。计算完每个头的注意力后，模型会将这些结果拼接起来，并通过线性变换生成最终的输出。这种方法通过融合多个注意力视角的信息，显著增强了BERT对上下文的理解能力，使其在自然语言处理任务中表现出色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 query、key 和 value 状态应用 transpose_for_scores 函数\n",
    "transposed_query_states = transpose_for_scores(query_states)\n",
    "transposed_key_states = transpose_for_scores(key_states)\n",
    "transposed_value_states = transpose_for_scores(value_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transposed_query_states shape:  torch.Size([1, 12, 12, 64])\n",
      "transposed_key_states shape:  torch.Size([1, 12, 12, 64])\n",
      "transposed_value_states shape:  torch.Size([1, 12, 12, 64])\n"
     ]
    }
   ],
   "source": [
    "print('transposed_query_states shape: ',transposed_query_states.shape)\n",
    "print('transposed_key_states shape: ',transposed_key_states.shape)\n",
    "print('transposed_value_states shape: ',transposed_value_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算注意力得分\n",
    "\n",
    "然后我们在自注意力机制中将查询（queries）和键（key）矩阵相乘：\n",
    "\n",
    "\t• 进行这个操作会产生一个得分，用来映射每个标记与序列中所有其他标记之间的关系。\n",
    "\t• 该得分表示每个标记的查询与每个其他标记的键对齐的程度。\n",
    "\t• 得出的注意力得分矩阵（称为 qk_per_token）的形状为 [12x12]，其中12表示输入序列中的标记数量。\n",
    "\n",
    "#### 从零实现注意力机制\n",
    "\n",
    "我们来加载transformer第一层的注意力头。\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/qkv.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "### 加载查询、键、值和输出矩阵\n",
    "\n",
    "当我们从模型中加载查询、键、值和输出矩阵时，我们注意到它们的形状为：\n",
    "\n",
    "\t• 查询（Query）：[768x768]\n",
    "\t• 键（Key）：[768x768]\n",
    "\t• 值（Value）：[768x768]\n",
    "\t• 输出（Output）：[768x768]\n",
    "\n",
    "乍一看，这可能显得有些不寻常，因为理想情况下，我们希望为每个注意力头分别拥有独立的q、k、v和o矩阵。\n",
    "\n",
    "然而，BERT的作者为了提高效率，将这些矩阵捆绑在一起，这样可以并行化注意力头的计算。\n",
    "\n",
    "现在，让我们展开这些捆绑的矩阵，以便分别访问每个注意力头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用缩放点积注意力（内置函数）计算注意力输出\n",
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    t_query_states,\n",
    "    t_key_states,\n",
    "    t_value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p= 0.0,\n",
    "# q_len > 1 是必要的，因为当 q_len == 1 时，AttentionMaskConverter.to_causal_4d 不会创建因果遮罩。\n",
    "    is_causal= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重塑并转置注意力输出以匹配预期的维度\n",
    "attn_output = attn_output.transpose(1, 2).reshape(1, 12, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取第一层注意力层的输出全连接层的权重和偏置\n",
    "o_layer0 = model[\"bert.encoder.layer.0.attention.output.dense.weight\"]\n",
    "o_layer0_bias = model['bert.encoder.layer.0.attention.output.dense.bias']\n",
    "# 通过应用全连接层变换计算最终的注意力输出状态\n",
    "ouput_states = torch.matmul(attn_output, o_layer0.T)+o_layer0_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0469,  0.1639, -0.0603,  ..., -0.1527,  0.0335,  0.1802],\n",
       "         [ 0.1023,  0.3846, -0.0411,  ..., -0.5442, -0.0701, -0.0260],\n",
       "         [-0.0404,  0.2899,  0.1311,  ..., -0.2009, -0.2455, -0.0298],\n",
       "         ...,\n",
       "         [ 0.2090,  0.3050,  0.0050,  ..., -0.2973, -0.1212, -0.0459],\n",
       "         [ 0.0831, -0.0208, -0.1554,  ..., -0.4028, -0.2934, -0.0784],\n",
       "         [ 0.3431, -0.1718,  0.0026,  ..., -0.3566, -0.2113,  0.0350]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ouput_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ouput_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取第一层注意力层输出的LayerNorm权重和偏置\n",
    "layer_0_output_norm_weight = model['bert.encoder.layer.0.attention.output.LayerNorm.weight']\n",
    "layer_0_output_norm_bias = model['bert.encoder.layer.0.attention.output.LayerNorm.bias']\n",
    "# 将LayerNorm应用于与输入嵌入结合的输出状态\n",
    "attention_output = torch.nn.functional.layer_norm(ouput_states+normalized_embeddings, [ouput_states.size(-1)], layer_0_output_norm_weight, layer_0_output_norm_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上采样\n",
    "\n",
    "在像BERT这样的Transformer模型中，上采样指的是通过增加表示的分辨率或维度的技术，以确保模型能够在各层之间捕捉并保留更多的详细信息。尽管像双线性插值和转置卷积这样的传统上采样方法更多与图像处理任务相关，这一概念在NLP模型中也有类似的应用，即通过增加或保持嵌入的维度来丰富特征空间。这个过程对于确保模型能够保持较高的信息密度至关重要，使得后续层（如前馈神经网络）能够处理更加丰富和复杂的表示，最终提升模型理解和生成细腻文本的能力。\n",
    "\n",
    "## 前馈神经网络\n",
    "\n",
    "在自注意力机制之后，BERT的每一层Transformer都会应用一个前馈神经网络来进一步处理嵌入。这一步对于引入非线性特征非常关键，能够使模型从输入数据中学习到更复杂的表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.1348, -2.2779, -3.1132,  ..., -2.4296, -3.1535, -2.9147],\n",
       "         [-1.7857, -1.4517, -3.6339,  ..., -1.9001, -2.6653,  0.6086],\n",
       "         [-3.0365, -0.8983, -4.2322,  ..., -2.2538, -2.9196, -2.1696],\n",
       "         ...,\n",
       "         [-1.7389, -2.1817, -4.1068,  ..., -1.2588, -1.4242, -1.0926],\n",
       "         [-2.9177, -1.5600, -3.8377,  ..., -1.5096, -2.2996, -1.7339],\n",
       "         [-3.7864, -2.3012, -3.4133,  ..., -2.6182, -2.6048, -3.1074]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取第一层Transformer中间密集层的权重和偏置\n",
    "layer_0_intermediate_dense_weight = model['bert.encoder.layer.0.intermediate.dense.weight']\n",
    "layer_0_intermediate_dense_bias = model['bert.encoder.layer.0.intermediate.dense.bias']\n",
    "# 计算中间密集层的输出\n",
    "intermediate_dense_output = torch.matmul(attention_output, layer_0_intermediate_dense_weight.T)+layer_0_intermediate_dense_bias\n",
    "intermediate_dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 3072])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_dense_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数\n",
    "\n",
    "在中间密集层计算出输出后，激活函数被应用以引入非线性到模型中。在BERT中，使用了GELU（高斯误差线性单元）激活函数。与更常见的ReLU（线性整流单元）不同，GELU是一个平滑、连续的函数，它将输出建模为概率近似，允许输出包含正值和负值。这一特性帮助BERT更好地捕捉输入数据中的细微关系，从而在各种自然语言处理任务中实现更好的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.6957e-03, -2.5889e-02, -2.8811e-03,  ..., -1.8364e-02,\n",
       "          -2.5440e-03, -5.1889e-03],\n",
       "         [-6.6208e-02, -1.0640e-01, -5.0749e-04,  ..., -5.4553e-02,\n",
       "          -1.0250e-02,  4.4340e-01],\n",
       "         [-3.6334e-03, -1.6575e-01, -4.8812e-05,  ..., -2.7281e-02,\n",
       "          -5.1157e-03, -3.2584e-02],\n",
       "         ...,\n",
       "         [-7.1338e-02, -3.1777e-02, -8.2369e-05,  ..., -1.3097e-01,\n",
       "          -1.0994e-01, -1.5000e-01],\n",
       "         [-5.1445e-03, -9.2632e-02, -2.3813e-04,  ..., -9.8983e-02,\n",
       "          -2.4687e-02, -7.1898e-02],\n",
       "         [-2.8956e-04, -2.4598e-02, -1.0957e-03,  ..., -1.1571e-02,\n",
       "          -1.1973e-02, -2.9320e-03]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将GELU激活函数应用于中间密集层的输出\n",
    "intermediate_output = torch.nn.functional.gelu(intermediate_dense_output)\n",
    "intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 3072])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下采样\n",
    "\n",
    "在像BERT这样的Transformer模型中，下采样指的是降低表示的分辨率或维度的技术，以确保模型能够更高效地处理信息，同时保留最关键的特征。尽管传统的下采样方法如池化或步幅卷积更常见于图像处理任务，这一概念可以在NLP模型中松散地应用，即通过减少嵌入的维度来简化特征空间。这个过程对于管理模型的计算负载至关重要，它允许后续层（如前馈神经网络）专注于最相关的信息。通过压缩数据，下采样帮助模型在复杂性和效率之间保持平衡，从而增强其处理大规模输入序列和生成准确预测的能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_0_intermediate_dense_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取第一层Transformer的输出密集层的权重和偏置\n",
    "layer_0_output_dense_weight = model['bert.encoder.layer.0.output.dense.weight']\n",
    "layer_0_output_dense_bias = model['bert.encoder.layer.0.output.dense.bias']\n",
    "# 计算第一层Transformer的最终密集层输出\n",
    "output_dense = torch.matmul(intermediate_output, layer_0_output_dense_weight.T)+layer_0_output_dense_bias\n",
    "output_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取第一层Transformer输出的LayerNorm权重和偏置\n",
    "layer_0_output_layernorm_weight = model['bert.encoder.layer.0.output.LayerNorm.weight']\n",
    "layer_0_output_layernorm_bias = model['bert.encoder.layer.0.output.LayerNorm.bias']\n",
    "# 将LayerNorm应用于密集层输出与原始注意力输出之和\n",
    "layer_norm(output_dense+attention_output, layer_0_output_layernorm_weight, layer_0_output_layernorm_bias, eps=1e-12).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过BERT的12层Transformer的前向传播\n",
    "\n",
    "接下来，我们对BERT模型的12层进行前向传播。\n",
    "\n",
    "每一层都应用多头自注意力机制来捕捉标记之间的关系，随后通过带有GELU激活函数的密集层引入非线性，并通过Layer Normalization来稳定输出。\n",
    "\n",
    "每层的输出都会传递到下一层，经过所有12层的处理后，我们得到一个最终的上下文化嵌入，准备用于下游任务，如分类或问答等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一次性全部处理\n",
    "<div>\n",
    "    <img src=\"images/12self_attention.png\" width=\"600px\"/>\n",
    "</div>\n",
    "没错，就是这样。我们之前做的所有事情，现在要一次性对每一层都进行处理。\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0698,  0.3857, -0.0319,  ..., -0.1526,  0.0446,  0.2253],\n",
       "         [-0.5065,  0.7490,  0.5782,  ..., -0.7123, -0.4653, -0.7150],\n",
       "         [-0.0510, -0.6486, -0.1926,  ..., -0.1028,  0.0320,  0.1524],\n",
       "         ...,\n",
       "         [ 0.8641,  0.3875,  0.5311,  ...,  0.2063,  0.1742,  0.1639],\n",
       "         [-0.4663, -0.5197, -0.2011,  ...,  0.6513,  0.0255, -0.1058],\n",
       "         [ 0.7993,  0.2471, -0.0632,  ...,  0.1179, -0.7094, -0.0537]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用输入层的归一化嵌入进行初始化\n",
    "layer_embedding_norm = normalized_embeddings\n",
    "# 遍历BERT中的所有12个Transformer层\n",
    "for layer in range(12):\n",
    "    # 提取当前层注意力机制的query、key和value的权重\n",
    "    q_layer = model[f\"bert.encoder.layer.{layer}.attention.self.query.weight\"]\n",
    "    k_layer = model[f\"bert.encoder.layer.{layer}.attention.self.key.weight\"]\n",
    "    v_layer = model[f\"bert.encoder.layer.{layer}.attention.self.value.weight\"]\n",
    "    \n",
    "    # 提取query、key和value的偏置\n",
    "    q_layer_bias = model[f'bert.encoder.layer.{layer}.attention.self.query.bias']\n",
    "    k_layer_bias = model[f'bert.encoder.layer.{layer}.attention.self.key.bias']\n",
    "    v_layer_bias = model[f'bert.encoder.layer.{layer}.attention.self.value.bias']\n",
    "    \n",
    "    # 通过应用相应的权重和偏置计算query、key和value状态\n",
    "    query_states = torch.matmul(layer_embedding_norm, q_layer.T)+q_layer_bias\n",
    "    key_states = torch.matmul(layer_embedding_norm, k_layer.T)+k_layer_bias\n",
    "    value_states = torch.matmul(layer_embedding_norm, v_layer.T)+v_layer_bias\n",
    "    \n",
    "    # 转置并重塑状态以进行多头注意力计算\n",
    "    t_query_states = transpose_for_scores(query_states)\n",
    "    t_key_states = transpose_for_scores(key_states)\n",
    "    t_value_states = transpose_for_scores(value_states)\n",
    "    \n",
    "    # 使用缩放点积注意力计算注意力输出\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        t_query_states,\n",
    "        t_key_states,\n",
    "        t_value_states,\n",
    "        attn_mask=None,\n",
    "        dropout_p= 0.0,\n",
    "        # q_len > 1 是必要的，因为当 q_len == 1 时，AttentionMaskConverter.to_causal_4d 不会创建因果遮罩。\n",
    "        is_causal= False,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).reshape(1, 12, 768)\n",
    "    \n",
    "    # 提取注意力输出变换的密集层权重和偏置\n",
    "    o_layer = model[f\"bert.encoder.layer.{layer}.attention.output.dense.weight\"]\n",
    "    o_layer_bias = model[f'bert.encoder.layer.{layer}.attention.output.dense.bias']\n",
    "    ouput_states = torch.matmul(attn_output, o_layer.T)+o_layer_bias\n",
    "    \n",
    "    # 提取注意力输出的LayerNorm权重和偏置\n",
    "    layer_output_norm_weight = model[f'bert.encoder.layer.{layer}.attention.output.LayerNorm.weight']\n",
    "    layer_output_norm_bias = model[f'bert.encoder.layer.{layer}.attention.output.LayerNorm.bias']\n",
    "    attention_output11 = layer_norm(ouput_states+layer_embedding_norm, layer_output_norm_weight, layer_output_norm_bias, eps=1e-12)\n",
    "\n",
    "    # 计算中间密集层的输出\n",
    "    layer_intermediate_dense_weight = model[f'bert.encoder.layer.{layer}.intermediate.dense.weight']\n",
    "    layer_intermediate_dense_bias = model[f'bert.encoder.layer.{layer}.intermediate.dense.bias']\n",
    "    intermediate_dense_output = torch.matmul(attention_output11, layer_intermediate_dense_weight.T)+layer_intermediate_dense_bias\n",
    "    \n",
    "    # 应用GELU激活函数\n",
    "    intermediate_output = torch.nn.functional.gelu(intermediate_dense_output)\n",
    "    \n",
    "    # 计算输出密集层的变换\n",
    "    layer_output_dense_weight = model[f'bert.encoder.layer.{layer}.output.dense.weight']\n",
    "    layer_output_dense_bias = model[f'bert.encoder.layer.{layer}.output.dense.bias']\n",
    "    output_dense = torch.matmul(intermediate_output, layer_output_dense_weight.T)+layer_output_dense_bias\n",
    "    \n",
    "    # 为当前层应用最终的LayerNorm\n",
    "    layer_output_layernorm_weight = model[f'bert.encoder.layer.{layer}.output.LayerNorm.weight']\n",
    "    layer_output_layernorm_bias = model[f'bert.encoder.layer.{layer}.output.LayerNorm.bias']\n",
    "    layer_embedding_norm = layer_norm(output_dense+attention_output11, layer_output_layernorm_weight, layer_output_layernorm_bias, eps=1e-12)\n",
    "\n",
    "\n",
    "# 所有12层Transformer后的最终输出\n",
    "    layer_embedding_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_embedding_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9308,  0.5810, -1.5488,  ..., -0.3969, -0.2049, -0.7886],\n",
       "         [-0.4524,  1.4434,  0.7572,  ...,  0.8591,  1.1784,  1.1476],\n",
       "         [ 0.1031,  0.8686,  0.9038,  ...,  1.2142,  0.3628,  1.5648],\n",
       "         ...,\n",
       "         [ 1.0442, -0.6966,  1.6925,  ...,  1.5027,  0.0384,  2.1948],\n",
       "         [-0.2648, -0.0293,  0.6970,  ...,  0.6369,  1.0992,  1.0511],\n",
       "         [ 0.1265, -1.1419,  0.2915,  ..., -0.2913,  0.0908,  0.3325]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取预测头中密集层的权重和偏置\n",
    "layer_intermediate_dense_weight = model['cls.predictions.transform.dense.weight']\n",
    "layer_intermediate_dense_bias = model['cls.predictions.transform.dense.bias']\n",
    "\n",
    "# 计算预测头中密集层的输出\n",
    "prediction_output = torch.matmul(layer_embedding_norm, layer_intermediate_dense_weight.T)+layer_intermediate_dense_bias\n",
    "prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6974,  1.0231, -0.0350,  ..., -0.3549, -0.0176,  0.0247],\n",
       "         [-4.3757,  1.5879, -1.3361,  ..., -1.1796,  0.3573,  0.4541],\n",
       "         [-3.5006, -0.2887, -0.2252,  ...,  0.9825, -2.4059,  3.3587],\n",
       "         ...,\n",
       "         [-0.4326, -4.0172,  2.9924,  ...,  1.8936, -3.3367,  5.7582],\n",
       "         [-2.9313, -2.1162, -0.0482,  ..., -0.5154,  1.6312,  1.7295],\n",
       "         [-0.8187, -2.2804,  0.7585,  ..., -2.2679, -0.4951,  1.3419]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对预测输出应用GELU激活函数\n",
    "prediction_act_dense_output = torch.nn.functional.gelu(prediction_output)\n",
    "\n",
    "# 提取预测输出的LayerNorm权重和偏置\n",
    "prediction_layernorm_weight = model['cls.predictions.transform.LayerNorm.weight']\n",
    "prediction_layernorm_bias = model['cls.predictions.transform.LayerNorm.bias']\n",
    "# 将LayerNorm应用于激活后的密集层输出\n",
    "prediction_dense_output = layer_norm(prediction_act_dense_output, prediction_layernorm_weight, prediction_layernorm_bias, eps=1e-12)\n",
    "prediction_dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dense_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取用于最终预测的解码器权重和偏置\n",
    "decoder_weight = model['cls.predictions.decoder.weight'] # 形状 [vocab_size, hidden_size]\n",
    "decoder_bias = model['cls.predictions.decoder.bias'] # 形状 [vocab_size]\n",
    "\n",
    "# 提取用于最终预测的解码器权重和偏置\n",
    "# if 'cls.predictions.bias' in model:\n",
    "#     decoder_bias += model['cls.predictions.bias']\n",
    "\n",
    "# 将最终输出投影到词汇空间以计算logits\n",
    "logits = torch.matmul(prediction_dense_output, decoder_weight.T) + decoder_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 30522])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取预测的标记 (词汇表中的索引)\n",
    "predictions = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2043, 1999, 4199, 1010, 2079, 2004, 1996,  103, 2079, 1012,  102]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets go！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. when in rome, do as the romans do..'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(predictions[0], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thank you\n",
    "\n",
    "结束了！希望你喜欢阅读这一切！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
