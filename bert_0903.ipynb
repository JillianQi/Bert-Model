{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT implemented from scratch\n",
    "\n",
    "In this file, I implemented BERT from scratch, focusing on the core concepts of transformer architecture, attention mechanisms, and matrix operations. BERT (Bidirectional Encoder Representations from Transformers) is a pivotal model in NLP that uses a bidirectional approach to understand the context of words in a sentence.\n",
    "\n",
    "<br>\n",
    "\n",
    "I will also demonstrate how to load tensors directly from the pre-trained BERT model provided by Hugging Face. Before running this file, you need to download the weights. Here is the official link to download the weights: [Hugging Face BERT](https://huggingface.co/bert-base-uncased)\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/all-steps.png\"/ width=800>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - pytorch\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - cpuonly\n",
      "    - pytorch\n",
      "    - torchaudio\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2024.7.4-~ --> pkgs/main/osx-arm64::certifi-2024.7.4-py312hca03da5_0 \n",
      "  conda              conda-forge::conda-24.7.1-py312h81bd7~ --> pkgs/main::conda-24.7.1-py312hca03da5_0 \n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Input Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "I won’t be implementing a BPE tokenizer here, but Andrej Karpathy has a neat implementation that you might find useful.\n",
    "<br>\n",
    "You can check out his implementation at this link: https://github.com/karpathy/minbpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "model_path=\"bert-base-uncased\"\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Inspecting Model Configuration and Weights\n",
    "\n",
    "Here, the model configuration and weights are loaded from a pretrained model file. This step ensures that the necessary parameters are ready for the forward pass.\n",
    "\n",
    "Normally, reading a model file depends on how the model classes are written and the variable names within them. \n",
    "However, since we are implementing BERT from scratch, we will read the file one tensor at a time, carefully examining the embedding layers, attention heads, and feed-forward networks that make up BERT’s architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. 使用 transformers 加载预训练模型\n",
    "\n",
    "# 加载模型配置和权重\n",
    "model = BertForMaskedLM.from_pretrained(model_path, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float32\n",
    "model['bert.embeddings.word_embeddings.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/jlpanc/anaconda3/envs/codef/STA/bert-base-uncased/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Model Parameters from Config\n",
    "We use this config to infer details about the model like:\n",
    "\n",
    "1. The model has 12 transformer layers.\n",
    "2. Each multi-head attention block has 12 heads.\n",
    "3. The vocabulary size is 30,522 tokens.\n",
    "4. The hidden size is 768 dimensions.\n",
    "5. The intermediate size (for the feed-forward layers) is 3072 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = config[\"hidden_size\"]\n",
    "n_layers = config[\"num_hidden_layers\"]\n",
    "n_heads = config[\"num_attention_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "norm_eps = config[\"layer_norm_eps\"]\n",
    "eps = config[\"layer_norm_eps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting text to tokens\n",
    "Here, we utilize the tiktoken library (which I believe is developed by OpenAI) as the tokenizer.\n",
    "<div>\n",
    "    <img src=\"images/embedding_layers.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenize the prompt into token IDs using the BERT tokenizer.\n",
    "prompt = \"When in Rome, do as the [MASK] do.\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "print(\"input_ids: \", input_ids)\n",
    "# 2. Create a tensor of token type IDs (all zeros, as we have only one sentence).\n",
    "token_type_ids = torch.zeros_like(input_ids)\n",
    "print(\"token_type_ids: \", token_type_ids)\n",
    "# 3. Convert the input_ids to a tensor (this step is redundant since it's already a tensor).\n",
    "input_ids = torch.tensor(input_ids)\n",
    "# 4. Calculate and print the number of tokens in the input prompt.\n",
    "q_len = len(input_ids)\n",
    "print(\"q_len: \", q_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Token Embeddings\n",
    "In this section, we convert input tokens into their corresponding embeddings using BERT’s pre-trained embedding layers. While BERT uses inbuilt neural network modules for this process, it’s crucial to understand the transformation.\n",
    "\n",
    "So, our [10x1] tokens are now transformed into [10x768], i.e., 12 embeddings (one for each token) of length 768.\n",
    "\n",
    "Note: Keep track of the tensor shapes throughout the process; it makes understanding the entire architecture much easier.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/embeddings_output.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the word embeddings from the pre-trained BERT model\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(model['bert.embeddings.word_embeddings.weight'])\n",
    "# 2. Load the position embeddings from the pre-trained BERT model\n",
    "position_embeddings = torch.nn.Embedding.from_pretrained(model['bert.embeddings.position_embeddings.weight'])\n",
    "# 3. Load the segment (token type) embeddings from the pre-trained BERT model\n",
    "segment_embeddings = torch.nn.Embedding.from_pretrained(model['bert.embeddings.token_type_embeddings.weight'])\n",
    "# embedding_layer.weight.data.copy_(model[\"model.embed_tokens.weight\"])\n",
    "token_embeddings_unnormalized = embedding_layer(input_ids)\n",
    "token_embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = input_ids.size(1)\n",
    "position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "position_embeds = position_embeddings(position_ids)\n",
    "position_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_embeds = segment_embeddings(token_type_ids)\n",
    "segment_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_unnormalized = token_embeddings_unnormalized+position_embeds+segment_embeds\n",
    "embeddings_unnormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Layer Normalization\n",
    "In BERT, we apply Layer Normalization after embedding layers and various other stages.\n",
    "Note: This step does not change the shape of the tensor; it just normalizes the values across the hidden dimension.\n",
    "\n",
    "### Things to keep in mind:\n",
    "1. Layer normalization prevents internal covariate shift by normalizing across the features.\n",
    "2. A small epsilon value (from the configuration) is added for numerical stability, preventing division by zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the First Layer of the Transformer\n",
    "\n",
    "#### Normalization\n",
    "In BERT, we start by applying layer normalization to the input of each transformer layer.\n",
    "\n",
    "You will notice that I access `encoder.layer.0` from the model dictionary (this corresponds to the first layer in BERT).\n",
    "After applying the normalization, the tensor shape remains the same as the input, [12x768], but the values are now normalized.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/layer_norm.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, gamma, beta, eps=1e-12):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    x (torch.Tensor): 输入张量，形状为 (batch_size, num_features)\n",
    "    gamma (torch.Tensor): 缩放参数，形状与x的最后一个维度相同\n",
    "    beta (torch.Tensor): 偏移参数，形状与x的最后一个维度相同\n",
    "    eps (float): 用于数值稳定性的常量（防止除以零）\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 经过LayerNorm后的输出\n",
    "    \"\"\"\n",
    "    # 计算均值和方差\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    \n",
    "    # 标准化\n",
    "    x_normalized = (x - mean) / torch.sqrt(variance + eps)\n",
    "    \n",
    "    # 缩放和偏移\n",
    "    out = gamma * x_normalized + beta\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the first transformer layer\n",
    "### Normalization\n",
    "You can see, after through layer0 from the dict extract from the model.\n",
    "\n",
    "The output tensor is still shape in [10*768] but normalized.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/embeding_norm1.png\", width=500>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LayerNorm and Dropout\n",
    "# layer_norm = torch.nn.LayerNorm(768, eps=1e-12)\n",
    "# layer_norm.weight.data = model['bert.embeddings.LayerNorm.weight']\n",
    "# layer_norm.bias.data = model['bert.embeddings.LayerNorm.bias']\n",
    "\n",
    "# Perform Layer Normalization\n",
    "normalized_embeddings = layer_norm(embeddings_unnormalized, model['bert.embeddings.LayerNorm.weight'], model['bert.embeddings.LayerNorm.bias'], eps=1e-12)\n",
    "\n",
    "# Apply Dropout\n",
    "# final_embeddings = dropout(normalized_embeddings)\n",
    "\n",
    "# Print results\n",
    "print(\"Embeddings after LayerNorm:\", normalized_embeddings)\n",
    "# print(\"Final Embeddings:\", final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Mechanism\n",
    "This section dives into how the self-attention mechanism is implemented in BERT, including the computation of Query, Key, and Value matrices, and how they contribute to generating attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the query, key, and value weights for the first attention layer from the BERT model\n",
    "q_layer0 = model[\"bert.encoder.layer.0.attention.self.query.weight\"]\n",
    "k_layer0 = model[\"bert.encoder.layer.0.attention.self.key.weight\"]\n",
    "v_layer0 = model[\"bert.encoder.layer.0.attention.self.value.weight\"]\n",
    "# Extract the biases for query, key, and value in the first attention layer\n",
    "q_layer0_bias = model['bert.encoder.layer.0.attention.self.query.bias']\n",
    "k_layer0_bias = model['bert.encoder.layer.0.attention.self.key.bias']\n",
    "v_layer0_bias = model['bert.encoder.layer.0.attention.self.value.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the query, key, and value states by multiplying the normalized embeddings with the respective weights and adding the bias\n",
    "query_states = torch.matmul(normalized_embeddings, q_layer0.T)+q_layer0_bias\n",
    "key_states = torch.matmul(normalized_embeddings, k_layer0.T)+k_layer0_bias\n",
    "value_states = torch.matmul(normalized_embeddings, v_layer0.T)+v_layer0_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to reshape and permute the states for multi-head attention\n",
    "def transpose_for_scores(x):\n",
    "    new_x_shape = x.size()[:-1] + (12, 64) # (num_attention_heads, attention_head_size)\n",
    "    x = x.view(new_x_shape) # Reshape the tensor to separate attention heads\n",
    "    return x.permute(0, 2, 1, 3) # Permute the tensor dimensions for attention computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transpose_for_scores function to the query, key, and value states\n",
    "t_query_states = transpose_for_scores(query_states)\n",
    "t_key_states = transpose_for_scores(key_states)\n",
    "t_value_states = transpose_for_scores(value_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Attention Score\n",
    "We then multiply the queries and key matrices in the process known as self-attention:\n",
    "\n",
    "- Performing this operation yields a score that maps the relationship between each token and every other token in the sequence. \n",
    "- This score indicates how well each token’s query aligns with every other token’s key. \n",
    "- The resulting attention score matrix (referred to as qk_per_token) has a shape of [12x12], where 12 represents the number of tokens in the input sequence.\n",
    "\n",
    "\n",
    "#### Attention Implemented from Scratch\n",
    "\n",
    "Let's load the attention heads of the first layer of the transformer.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/qkv.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "### Loading the Query, Key, Value, and Output Matrices\n",
    "\n",
    "When we load the query, key, value, and output matrices from the model, we notice that their shapes are:\n",
    "- Query: [768x768]\n",
    "- Key: [768x768]\n",
    "- Value: [768x768]\n",
    "- Output: [768x768]\n",
    "\n",
    "At first glance, this might seem unusual because ideally, we would want separate q, k, v, and o matrices for each attention head individually.\n",
    "\n",
    "However, BERT’s authors bundled these matrices together for efficiency—it allows parallelization of the attention head calculations.\n",
    "\n",
    "Now, let's unwrap these bundled matrices to access each attention head individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the attention output using scaled dot-product attention (built-in function)\n",
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    t_query_states,\n",
    "    t_key_states,\n",
    "    t_value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p= 0.0,\n",
    "    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "    is_causal= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and transpose the attention output to match expected dimensions\n",
    "attn_output = attn_output.transpose(1, 2).reshape(1, 12, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the output dense layer weights and biases for the first attention layer\n",
    "o_layer0 = model[\"bert.encoder.layer.0.attention.output.dense.weight\"]\n",
    "o_layer0_bias = model['bert.encoder.layer.0.attention.output.dense.bias']\n",
    "# Compute the final attention output states by applying the dense layer transformation\n",
    "ouput_states = torch.matmul(attn_output, o_layer0.T)+o_layer0_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouput_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the LayerNorm weights and biases for the first attention layer's output\n",
    "layer_0_output_norm_weight = model['bert.encoder.layer.0.attention.output.LayerNorm.weight']\n",
    "layer_0_output_norm_bias = model['bert.encoder.layer.0.attention.output.LayerNorm.bias']\n",
    "# Apply LayerNorm to the output states combined with the input embeddings\n",
    "attention_output = torch.nn.functional.layer_norm(ouput_states+normalized_embeddings, [ouput_states.size(-1)], layer_0_output_norm_weight, layer_0_output_norm_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "In the context of transformer models like BERT, upsampling refers to techniques that increase the resolution or dimensionality of representations, ensuring that the model can capture and retain more detailed information across different layers. Although traditional upsampling methods like bilinear interpolation and transposed convolution are more commonly associated with image processing tasks, the concept can be loosely related to operations in NLP models where the dimensionality of embeddings is increased or preserved to enrich the feature space. This process is crucial in enabling the model to maintain high levels of information density, allowing subsequent layers, such as the feed-forward neural network, to process richer and more complex representations, ultimately improving the model’s ability to understand and generate nuanced text.\n",
    "\n",
    "## Feed-Forward Neural Network\n",
    "After the self-attention mechanism, each transformer layer in BERT applies a feed-forward neural network to further process the embeddings. This step is crucial for introducing non-linearity and enabling the model to learn complex representations from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights and biases for the intermediate dense layer of the first transformer layer\n",
    "layer_0_intermediate_dense_weight = model['bert.encoder.layer.0.intermediate.dense.weight']\n",
    "layer_0_intermediate_dense_bias = model['bert.encoder.layer.0.intermediate.dense.bias']\n",
    "# Compute the output of the intermediate dense layer\n",
    "intermediate_dense_output = torch.matmul(attention_output, layer_0_intermediate_dense_weight.T)+layer_0_intermediate_dense_bias\n",
    "intermediate_dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dense_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "After the intermediate dense layer computes its output, an activation function is applied to introduce non-linearity into the model. In BERT, the GELU (Gaussian Error Linear Unit) activation function is used. Unlike the more common ReLU (Rectified Linear Unit), GELU is a smooth, continuous function that models the output as a probabilistic approximation, allowing for both positive and negative values. This property helps BERT to better capture the nuanced relationships in the input data, leading to improved performance on a variety of NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the GELU activation function to the intermediate dense output\n",
    "intermediate_output = torch.nn.functional.gelu(intermediate_dense_output)\n",
    "intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0_intermediate_dense_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights and biases for the output dense layer of the first transformer layer\n",
    "layer_0_output_dense_weight = model['bert.encoder.layer.0.output.dense.weight']\n",
    "layer_0_output_dense_bias = model['bert.encoder.layer.0.output.dense.bias']\n",
    "# Compute the final dense layer output for the first transformer layer\n",
    "output_dense = torch.matmul(intermediate_output, layer_0_output_dense_weight.T)+layer_0_output_dense_bias\n",
    "output_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the LayerNorm weights and biases for the output of the first transformer layer\n",
    "layer_0_output_layernorm_weight = model['bert.encoder.layer.0.output.LayerNorm.weight']\n",
    "layer_0_output_layernorm_bias = model['bert.encoder.layer.0.output.LayerNorm.bias']\n",
    "# Apply LayerNorm to the sum of the dense layer output and the original attention output\n",
    "layer_norm(output_dense+attention_output, layer_0_output_layernorm_weight, layer_0_output_layernorm_bias, eps=1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass Through BERT’s 12 Transformer Layers\n",
    "Next, we run a forward pass through all 12 layers of the BERT model. \n",
    "\n",
    "Each layer applies multi-head self-attention to capture token relationships, followed by a dense layer with GELU activation for non-linearity, and then Layer Normalization to stabilize the output. \n",
    "\n",
    "The output from each layer is passed to the next, and after processing all 12 layers, we obtain a final contextualized embedding ready for downstream tasks like classification or question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## god, everything all at once\n",
    "<div>\n",
    "    <img src=\"images/12self_attention.png\" width=\"600px\"/>\n",
    "</div>\n",
    "yep, this is it. everything we did before, all at once, for every single layer.\n",
    "<br>\n",
    "\n",
    "### have fun reading :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with the normalized embeddings from the input layer\n",
    "layer_embedding_norm = normalized_embeddings\n",
    "# Iterate through all 12 transformer layers in BERT\n",
    "for layer in range(12):\n",
    "    # Extract query, key, and value weights for the current layer's attention mechanism\n",
    "    q_layer = model[f\"bert.encoder.layer.{layer}.attention.self.query.weight\"]\n",
    "    k_layer = model[f\"bert.encoder.layer.{layer}.attention.self.key.weight\"]\n",
    "    v_layer = model[f\"bert.encoder.layer.{layer}.attention.self.value.weight\"]\n",
    "    \n",
    "    # Extract biases for the query, key, and value\n",
    "    q_layer_bias = model[f'bert.encoder.layer.{layer}.attention.self.query.bias']\n",
    "    k_layer_bias = model[f'bert.encoder.layer.{layer}.attention.self.key.bias']\n",
    "    v_layer_bias = model[f'bert.encoder.layer.{layer}.attention.self.value.bias']\n",
    "    \n",
    "    # Compute query, key, and value states by applying the respective weights and biases\n",
    "    query_states = torch.matmul(layer_embedding_norm, q_layer.T)+q_layer_bias\n",
    "    key_states = torch.matmul(layer_embedding_norm, k_layer.T)+k_layer_bias\n",
    "    value_states = torch.matmul(layer_embedding_norm, v_layer.T)+v_layer_bias\n",
    "    \n",
    "    # Transpose and reshape states for multi-head attention computation\n",
    "    t_query_states = transpose_for_scores(query_states)\n",
    "    t_key_states = transpose_for_scores(key_states)\n",
    "    t_value_states = transpose_for_scores(value_states)\n",
    "    \n",
    "    # Compute the attention output using scaled dot-product attention\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        t_query_states,\n",
    "        t_key_states,\n",
    "        t_value_states,\n",
    "        attn_mask=None,\n",
    "        dropout_p= 0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal= False,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).reshape(1, 12, 768)\n",
    "    \n",
    "    # Extract the dense layer weights and biases for the attention output transformation\n",
    "    o_layer = model[f\"bert.encoder.layer.{layer}.attention.output.dense.weight\"]\n",
    "    o_layer_bias = model[f'bert.encoder.layer.{layer}.attention.output.dense.bias']\n",
    "    ouput_states = torch.matmul(attn_output, o_layer.T)+o_layer_bias\n",
    "    \n",
    "    # Extract LayerNorm weights and biases for the attention output\n",
    "    layer_output_norm_weight = model[f'bert.encoder.layer.{layer}.attention.output.LayerNorm.weight']\n",
    "    layer_output_norm_bias = model[f'bert.encoder.layer.{layer}.attention.output.LayerNorm.bias']\n",
    "    attention_output11 = layer_norm(ouput_states+layer_embedding_norm, layer_output_norm_weight, layer_output_norm_bias, eps=1e-12)\n",
    "\n",
    "    # Compute intermediate dense layer output\n",
    "    layer_intermediate_dense_weight = model[f'bert.encoder.layer.{layer}.intermediate.dense.weight']\n",
    "    layer_intermediate_dense_bias = model[f'bert.encoder.layer.{layer}.intermediate.dense.bias']\n",
    "    intermediate_dense_output = torch.matmul(attention_output11, layer_intermediate_dense_weight.T)+layer_intermediate_dense_bias\n",
    "    \n",
    "    # Apply GELU activation function\n",
    "    intermediate_output = torch.nn.functional.gelu(intermediate_dense_output)\n",
    "    \n",
    "    # Compute output dense layer transformation\n",
    "    layer_output_dense_weight = model[f'bert.encoder.layer.{layer}.output.dense.weight']\n",
    "    layer_output_dense_bias = model[f'bert.encoder.layer.{layer}.output.dense.bias']\n",
    "    output_dense = torch.matmul(intermediate_output, layer_output_dense_weight.T)+layer_output_dense_bias\n",
    "    \n",
    "    # Apply final LayerNorm for the current layer\n",
    "    layer_output_layernorm_weight = model[f'bert.encoder.layer.{layer}.output.LayerNorm.weight']\n",
    "    layer_output_layernorm_bias = model[f'bert.encoder.layer.{layer}.output.LayerNorm.bias']\n",
    "    layer_embedding_norm = layer_norm(output_dense+attention_output11, layer_output_layernorm_weight, layer_output_layernorm_bias, eps=1e-12)\n",
    "\n",
    "\n",
    "# Final output after all 12 layers of the transformer\n",
    "layer_embedding_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_embedding_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights and biases for the dense layer in the prediction head\n",
    "layer_intermediate_dense_weight = model['cls.predictions.transform.dense.weight']\n",
    "layer_intermediate_dense_bias = model['cls.predictions.transform.dense.bias']\n",
    "\n",
    "# Compute the output of the dense layer in the prediction head\n",
    "prediction_output = torch.matmul(layer_embedding_norm, layer_intermediate_dense_weight.T)+layer_intermediate_dense_bias\n",
    "prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GELU activation to the prediction output\n",
    "prediction_act_dense_output = torch.nn.functional.gelu(prediction_output)\n",
    "\n",
    "# Extract LayerNorm weights and biases for the prediction output\n",
    "prediction_layernorm_weight = model['cls.predictions.transform.LayerNorm.weight']\n",
    "prediction_layernorm_bias = model['cls.predictions.transform.LayerNorm.bias']\n",
    "# Apply LayerNorm to the activated dense output\n",
    "prediction_dense_output = layer_norm(prediction_act_dense_output, prediction_layernorm_weight, prediction_layernorm_bias, eps=1e-12)\n",
    "prediction_dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'cls.predictions.bias', , , 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract decoder weights and biases for final prediction\n",
    "decoder_weight = model['cls.predictions.decoder.weight'] # 形状 [vocab_size, hidden_size]\n",
    "decoder_bias = model['cls.predictions.decoder.bias'] # 形状 [vocab_size]\n",
    "\n",
    "# Extract decoder weights and biases for final prediction\n",
    "# if 'cls.predictions.bias' in model:\n",
    "#     decoder_bias += model['cls.predictions.bias']\n",
    "\n",
    "# Project the final output to the vocabulary space to compute logits\n",
    "logits = torch.matmul(prediction_dense_output, decoder_weight.T) + decoder_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取预测的标记 (词汇表中的索引)\n",
    "predictions = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(predictions[0], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we now multiply the query weights with the token embedding, to recive a query for the token\n",
    "here you can see the resulting shape is [12x768], this is because we have 12 tokens and for each token there is a 128 length query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keys (almost the same as queries)\n",
    "\n",
    "#### Here are the key points to remember:\n",
    "\n",
    "- Key vectors are also of dimension 768, corresponding to the token embeddings.\n",
    "- The weights for the keys are shared across multiple attention heads to optimize computation, reducing the number of parameters.\n",
    "- Like query vectors, key vectors are adjusted with positional information to encode the relative position of tokens in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### at this stage now have both the rotated values of queries and keys, for each token. \n",
    "\n",
    "each of the queries and keys are now of shape [12x768]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Attention Score\n",
    "We then multiply the queries and key matrices in the process known as self-attention:\n",
    "\n",
    "- Performing this operation yields a score that maps the relationship between each token and every other token in the sequence. \n",
    "- This score indicates how well each token’s query aligns with every other token’s key. \n",
    "- The resulting attention score matrix (referred to as qk_per_token) has a shape of [12x12], where 12 represents the number of tokens in the input sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention\n",
    "After calculating the attention scores, the resultant attention vector is obtained by multiplying the attention scores with the value vectors for each token. The resulting attention output for each token has a shape of [12x768]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-head attention\n",
    "We now have the attention output for the first head in the first layer.\n",
    "\n",
    "Next, we will run a loop to perform the same calculations for each attention head in the first layer, ensuring that all heads contribute to the final attention output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the QKV attention matrices for all 12 heads in the first layer. Next, we will concatenate these attention outputs into a single matrix of size [12x768], where 12 is the number of tokens, and 768 is the combined dimension from all heads.\n",
    "\n",
    "We’re almost at the finish line! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight matrix, one of the final steps\n",
    "In BERT, after the self-attention mechanism within a transformer layer, the attention output is typically multiplied by a weight matrix for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the change in the embedding value after attention, which should be added to the original token embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we normalize and then run a feed forward neural network through the embedding delta\n",
    "After normalization, BERT feeds the embedding delta into a feed-forward neural network. This step helps the model capture more complex relationships and introduces non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the ff weights and implementing the feed forward network\n",
    "In BERT, the feed-forward neural network usually consists of two linear transformations with a non-linear activation function like GELU. This architecture is standard in transformer models and effectively adds the necessary non-linearity for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WE FINALLY HAVE NEW EDITED EMBEDDINGS FOR EACH TOKEN AFTER THE FIRST LAYER\n",
    "After processing, the embeddings now carry more contextual information. With each additional transformer layer, BERT encodes more complex queries, eventually producing an embedding that captures deep semantic information for each token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0_embedding = embedding_after_edit+output_after_feedforward\n",
    "layer_0_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = model[\"model.layers.0.mlp.gate_proj.weight\"]\n",
    "w3 = model[\"model.layers.0.mlp.up_proj.weight\"]\n",
    "w2 = model[\"model.layers.0.mlp.down_proj.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = token_embeddings_unnormalized\n",
    "for layer in range(n_layers):\n",
    "    qkv_attention_store = []\n",
    "    layer_embedding_norm = rms_norm(final_embedding, model[f\"model.layers.{layer}.input_layernorm.weight\"])\n",
    "    # layer_embedding_norm = rms_norm(final_embedding, model[f\"layers.{layer}.attention_norm.weight\"])\n",
    "    q_layer = model[f\"model.layers.{layer}.self_attn.q_proj.weight\"]\n",
    "    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)\n",
    "    k_layer = model[f\"model.layers.{layer}.self_attn.k_proj.weight\"]\n",
    "    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)\n",
    "    v_layer = model[f\"model.layers.{layer}.self_attn.v_proj.weight\"]\n",
    "    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)\n",
    "    w_layer = model[f\"model.layers.{layer}.self_attn.o_proj.weight\"]\n",
    "    for head in range(n_heads):\n",
    "        q_layer_head = q_layer[head]\n",
    "        k_layer_head = k_layer[head//4]\n",
    "        v_layer_head = v_layer[head//4]\n",
    "        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)\n",
    "        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)\n",
    "        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)\n",
    "        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)\n",
    "        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
    "        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
    "        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
    "        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
    "        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
    "        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5\n",
    "        mask = torch.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(\"-inf\"))\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        qk_per_token_after_masking = qk_per_token + mask\n",
    "        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)\n",
    "        qk_per_token_after_masking_after_softmax_fl32 = qk_per_token_after_masking_after_softmax.type(v_per_token.dtype) ## extra\n",
    "        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax_fl32, v_per_token)\n",
    "        qkv_attention_store.append(qkv_attention)\n",
    "\n",
    "    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)\n",
    "    w_layer = model[f\"model.layers.{layer}.self_attn.o_proj.weight\"]\n",
    "    w_layer_bf16 = w_layer.type(stacked_qkv_attention.dtype)## extra\n",
    "    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer_bf16.T)\n",
    "    embedding_after_edit = final_embedding + embedding_delta\n",
    "    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f\"model.layers.{layer}.post_attention_layernorm.weight\"])\n",
    "    w1 = model[f\"model.layers.{layer}.mlp.gate_proj.weight\"]\n",
    "    w2 = model[f\"model.layers.{layer}.mlp.down_proj.weight\"]\n",
    "    w3 = model[f\"model.layers.{layer}.mlp.up_proj.weight\"]\n",
    "    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
    "    final_embedding = embedding_after_edit+output_after_feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Embedding Representation\n",
    "After passing through all transformer layers, BERT generates a final embedding that encapsulates the contextual information for each token. This embedding represents the model’s best understanding of the input, making it well-equipped to predict the next token.\n",
    "\n",
    "The shape of the embedding remains consistent with the original token embeddings, where the first dimension is the number of tokens and the second dimension is the embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = rms_norm(final_embedding, model[\"model.norm.weight\"])\n",
    "final_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Prediction and Output\n",
    "Once all transformer layers have processed the input, BERT converts the final embeddings into predictions by mapping them to the model’s vocabulary space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the Embeddings into Token Predictions\n",
    "We use the output projection layer (language modeling head) to translate the final embeddings into token predictions. This step is crucial for generating meaningful outputs, such as the next word in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Embedding of the Last Token to Predict the Next Value\n",
    "In BERT, the final prediction for the next token is derived from the embedding of the last token in the sequence. The logits produced represent the model’s confidence across the vocabulary, allowing it to select the most likely next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"model.norm.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.matmul(final_embedding[-1], model[\"lm_head.weight\"].T)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IM HYPING YOU UP, this is the last cell of code, hopefully you had fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = torch.argmax(logits, dim=-1)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = torch.argmax(logits, dim=-1)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets go！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([next_token.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[next_token.item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thank you\n",
    "\n",
    "This is the end. Hopefully you enjoyed reading it!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
